{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONSTRUIR UNA RED NEURONAL PASO A PASO\n",
    "\n",
    "- EN ESTE NOTEBOOK IMPLEMENTREMOS TODAS LAS FUNCIONES REQUERIDAS PARA CONSTRUIR UNA RED NEURONAL PROFUNDA.\n",
    "- EN EL SIGUIENTE EJERCICIO SE USARÁN ESTAS FUNCIONES PARA CONSTRUIR UNA RED NEURONAL PARA LA CLASIFICACIÓN DE IMÁGENES.\n",
    "\n",
    "**APRENDERÁS A**:\n",
    "- Usarás unidades no lineales como la función Relu para mejorar el modelo.\n",
    "- Construir una red neuronal profunda con más de una capa oculta.\n",
    "- Implementar una clase en python que permita construir una red neuronal facilmente.\n",
    "\n",
    "**Notation**:\n",
    "- El superíndice $[l]$ denota una cantidad asociada con la capa $l^{th}$. \n",
    "    - Ejemplo: $a^{[L]}$ es la capa de activación $L^{th}$. $W^{[L]}$ y $b^{[L]}$ sn los parámetros de la capa $L^{th}$.\n",
    "- El superíndice $(i)$ denota una canrtidad asociada al ejemplo $i^{th}$. \n",
    "    - Ejemplo: $x^{(i)}$ es el ejemplo $i^{th}$.\n",
    "- El subíndice $i$ denota la entrada $i^{th}$ de un vector.\n",
    "    - Ejemplo: $a^{[l]}_i$ denota la entrada $i^{th}$ de las activaciones de la capa $l^{th}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Paquetes\n",
    "\n",
    "- [numpy](www.numpy.org).\n",
    "- testCases casos para corroborar l funcionamiento de las funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from testCases_v4a import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Ruta de implementación\n",
    "\n",
    "Para construir tu red neuronal, implementarás varias \"funciones de ayuda\". Estas funciones serán usadas posteriormente para construir una red neuronal de dos o varias capas. La siguiente es una lista de las funciones a implementar:\n",
    "\n",
    "- Inicializar los parámetros de una red neuronal de 2 a más capas.\n",
    "- Implementar el módulo de \"forward propagation\".\n",
    "     - Completa la parte LINEAL de cada capa (dando como resultado $Z^{[l]}$).\n",
    "     - Implementar las funciones de activación(relu/sigmoid).\n",
    "     - Combinar los dos pasos anteriores en una nueva función [LINEAR->ACTIVATION].\n",
    "     - Apilar la función [LINEAR->RELU]  L-1 veces (para las capas 1 hasta L-1) y agregar una función [LINEAR->SIGMOID] al final (for the final layer $L$). esto nos dará el modelo para el \"forward propagation\".\n",
    "- Calcular la pérdida o costo.\n",
    "- Implementar el módulo de \"backward propagation\".\n",
    "    - Completar la parte lineal de un paso del \"backward propagation\".\n",
    "    - Implementar el gradiente de las funciones de activación (relu_backward/sigmoid_backward) \n",
    "    - Combinar los pasos anteriores en una función del gradiente [LINEAR->ACTIVATION] backward.\n",
    "    - Apilar [LINEAR->RELU] backward L-1 veces y agregar [LINEAR->SIGMOID] backward para generar el modelo de \"backward propagation\"\n",
    "- Finalmente actualizar los parámetros.\n",
    "\n",
    "<img src=\"images/final outline.png\" style=\"width:800px;height:500px;\">\n",
    "<caption><center> **Figure 1**</center></caption><br>\n",
    "\n",
    "\n",
    "**Nota** para cada función \"forward\" existe una función correspondiente \"backward\". Por esta razón en cada paso del módulo \"forward\" es necesario guardar los valores en una \"caché\". Los valores guardados en la caché serán de utilidad para calcular los gradientes. En el módulo de \"backpropagation\" se usarán dichos valores para calcular los gradientes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Initialización\n",
    "\n",
    "Escribiremos dos funciones de ayuda que inicializarán klos parámetros del modelo. La primera función será usada para inicializar los parámetros de un modelo de dos capas. La segunda función generalizará esta inicialización para $L$ capas.\n",
    "\n",
    "### 3.1 - Red neuronal de 2 capas\n",
    "\n",
    "**Ejercicio**: Crea e inicializa los parámetros para un modelos de dos capas.\n",
    "\n",
    "**Instrucciones**:\n",
    "- La estructura del modelo es: *LINEAR -> RELU -> LINEAR -> SIGMOID*. \n",
    "- Usar inicialización aleatoria para las matrices de pesos. Usar `np.random.randn(shape)*0.01` con la forma adecuada.\n",
    "- Usar incialización en ceros para los vectores \"b\" (bias). Usar `np.zeros(shape)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argumentos:\n",
    "    n_x -- tamaño de la capa de entrada\n",
    "    n_h -- tamaño de la capa oculta\n",
    "    n_y -- tamaño de la capa de salida (predicción)\n",
    "    \n",
    "    Regresa:\n",
    "    parameters -- Diccionario de python que contenga los parámetros:\n",
    "                    W1 -- matriz de pesos de la forma (n_h, n_x)\n",
    "                    b1 -- vector \"bias\" de la forma (n_h, 1)\n",
    "                    W2 -- matriz de pesos de la forma (n_y, n_h)\n",
    "                    b2 -- vector \"bias\" de la forma (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    ### INICIO ###\n",
    "    W1 = np.random.randn(n_h,n_x)*(0.01)\n",
    "    b1 = np.zeros((n_h,1))\n",
    "\n",
    "    W2 = np.random.randn(n_y,n_h) * (0.01)\n",
    "    b2 = np.zeros((n_y,1))\n",
    "  \n",
    "    ### FINAL ###\n",
    "    \n",
    "    assert(W1.shape == (n_h, n_x))\n",
    "    assert(b1.shape == (n_h, 1))\n",
    "    assert(W2.shape == (n_y, n_h))\n",
    "    assert(b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.01624345 -0.00611756 -0.00528172]\n",
      " [-0.01072969  0.00865408 -0.02301539]]\n",
      "b1 = [[0.]\n",
      " [0.]]\n",
      "W2 = [[ 0.01744812 -0.00761207]]\n",
      "b2 = [[0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters(3,2,1)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salida Esperada**:\n",
    "       \n",
    "<table style=\"width:80%\">\n",
    "  <tr>\n",
    "    <td> **W1** </td>\n",
    "    <td> [[ 0.01624345 -0.00611756 -0.00528172]\n",
    " [-0.01072969  0.00865408 -0.02301539]] </td> \n",
    "  </tr>\n",
    "\n",
    "  <tr>\n",
    "    <td> **b1**</td>\n",
    "    <td>[[ 0.]\n",
    " [ 0.]]</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**W2**</td>\n",
    "    <td> [[ 0.01744812 -0.00761207]]</td>\n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td> **b2** </td>\n",
    "    <td> [[ 0.]] </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**: Implementar la inicialización para una red neuronal de $ L $ capas. \n",
    "\n",
    "**Instrucciones**:\n",
    "- La estructura del modelo es *[LINEAR -> RELU] $ \\times$ (L-1) -> LINEAR -> SIGMOID*. es decir $L-1$ capas usando la función de activaxción ReLu, seguida por una capa de salida usando la función sigmoid.\n",
    "- Usar inicialización aleatoria para las matrices de pesos. Usar `np.random.randn(shape)*0.01` con la forma adecuada.\n",
    "- Usar incialización en ceros para los vectores \"b\" (bias). Usar `np.zeros(shape)`.\n",
    "- Introduciremos $n^{[l]}$, el número de unidades en diferentes capas, en la variable tipo lista`layer_dims`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Argumentos:\n",
    "    layer_dims -- Lista con las dimensiones de cada capa de la red neuronal\n",
    "    \n",
    "    Regresa:\n",
    "    parameters -- Diccionario de python con los parámetros \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- matriz de pesos de la forma (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- vector \"bias\" de la forma (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        ### INICIO ###\n",
    "        parameters['W'+ str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1])* 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "       \n",
    "        ### FIN ###\n",
    "\n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n",
      " [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n",
      " [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n",
      " [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[-0.01185047 -0.0020565   0.01486148  0.00236716]\n",
      " [-0.01023785 -0.00712993  0.00625245 -0.00160513]\n",
      " [-0.00768836 -0.00230031  0.00745056  0.01976111]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters_deep([5,4,3])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**:\n",
    "       \n",
    "<table style=\"width:80%\">\n",
    "  <tr>\n",
    "    <td> **W1** </td>\n",
    "    <td>[[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n",
    " [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n",
    " [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n",
    " [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**b1** </td>\n",
    "    <td>[[ 0.]\n",
    " [ 0.]\n",
    " [ 0.]\n",
    " [ 0.]]</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**W2** </td>\n",
    "    <td>[[-0.01185047 -0.0020565   0.01486148  0.00236716]\n",
    " [-0.01023785 -0.00712993  0.00625245 -0.00160513]\n",
    " [-0.00768836 -0.00230031  0.00745056  0.01976111]]</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**b2** </td>\n",
    "    <td>[[ 0.]\n",
    " [ 0.]\n",
    " [ 0.]]</td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Módulo de propagación hacia adelante \"forward\"\n",
    "\n",
    "### 4.1 - Linear Forward \n",
    "Implementar las siguientes funciones\n",
    "\n",
    "- LINEAR\n",
    "- LINEAR -> ACTIVATION donde ACTIVATION será ReLU o Sigmoid. \n",
    "- [LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID (modelo completo)\n",
    "\n",
    "El módulo de propagación hacia adelante (vectorizado sobre todos los ejemplos) calcula las siguientes ecuaciones:\n",
    "\n",
    "$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\\tag{4}$$\n",
    "\n",
    "donde $A^{[0]} = X$. \n",
    "\n",
    "**Ejercicio**: Construir la parte lineal de la propagación hacia adelante.\n",
    "\n",
    "**Recordatorio**:\n",
    "La representación matemática de esta unidad es $Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}$. Puedes usar `np.dot()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implementar la parte lineal de la propagación hacia adelante.\n",
    "\n",
    "    Argumentos:\n",
    "    A -- activaciones de la capa anterior\n",
    "    W -- matriz de pesos\n",
    "    b -- vector \"bias\"\n",
    "\n",
    "    Regresa:\n",
    "    Z -- la entrada de la función de activación, también llamado parámetro de \"preactivación\".\n",
    "    cache -- una variable tipo \"tuple\"  con \"A\", \"W\" and \"b\" ; guardada para calcular de manera más eficiente los gradientes en la propagación hacia atrás.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### INICIO ###\n",
    "    Z = W @ A + b\n",
    "    Z = np.matmul(W, A)\n",
    "    Z = np.dot(W, A) + b\n",
    "    \n",
    "    ### FIN ###\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = [[ 3.26295337 -1.23429987]]\n"
     ]
    }
   ],
   "source": [
    "A, W, b = linear_forward_test_case()\n",
    "\n",
    "Z, linear_cache = linear_forward(A, W, b)\n",
    "print(\"Z = \" + str(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**:\n",
    "\n",
    "<table style=\"width:35%\">\n",
    "  \n",
    "  <tr>\n",
    "    <td> **Z** </td>\n",
    "    <td> [[ 3.26295337 -1.23429987]] </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - Linear-Activation Forward\n",
    "\n",
    "Usaremos dos posibles funciones de activación:\n",
    "\n",
    "- **Sigmoid**: $\\sigma(Z) = \\sigma(W A + b) = \\frac{1}{ 1 + e^{-(W A + b)}}$\n",
    "\n",
    "- **ReLU**: la fórmula matemática de la función ReLu es $A = RELU(Z) = max(0, Z)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "\n",
    "    A = np.maximum(0,Z)\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    cache = Z \n",
    "    return A, cache\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para mayor conveniencia agruparemos las dos funciones (Linear y Activation) en una función (LINEAR->ACTIVATION).\n",
    "\n",
    "**Ejercicio**: Implementar la función de propagación *LINEAR->ACTIVATION*. Esto es: $A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} +b^{[l]})$ en donde la función de activación puede ser sigmoid() or relu()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implementar la función de propagación *LINEAR->ACTIVATION*.\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- Activaciones de la capa anterior\n",
    "    W -- Matriz de pesos de la capa actual\n",
    "    b -- Vector de \"bias\" de la capa actual\n",
    "    activation -- La activación que se usará en esta capa: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- La salida de la función de activación\n",
    "    cache -- una tupla de python que contenga \"linear_cache\" y \"activation_cache\";\n",
    "             almacenados para calcular la propagación hacia atrás de manera más eficiente\n",
    "    \"\"\"\n",
    "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "    if activation == \"sigmoid\":\n",
    "        # Entradas: \"A_prev, W, b\". Salidas: \"A, activation_cache\".\n",
    "        ### Inicio ###\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "        \n",
    "        ### FIN ###\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        # Entradas: \"A_prev, W, b\". Salidas: \"A, activation_cache\".\n",
    "        ### INICIO ###\n",
    "        A, activation_cache = relu(Z)\n",
    "        \n",
    "        ### FIN ###\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Con sigmoid: A = [[0.96890023 0.11013289]]\n",
      "Con ReLU: A = [[3.43896131 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "A_prev, W, b = linear_activation_forward_test_case()\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"sigmoid\")\n",
    "print(\"Con sigmoid: A = \" + str(A))\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"relu\")\n",
    "print(\"Con ReLU: A = \" + str(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salida Esperada**:\n",
    "       \n",
    "<table style=\"width:35%\">\n",
    "  <tr>\n",
    "    <td> **With sigmoid: A ** </td>\n",
    "    <td > [[ 0.96890023  0.11013289]]</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> **With ReLU: A ** </td>\n",
    "    <td > [[ 3.43896131  0.        ]]</td> \n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) L-Layer Model \n",
    "\n",
    "Para hacer aún más práctica la implementación de la rede neuronal de $L$-capas, se necesitará una función que replique la anterior (`linear_activation_forward` with RELU) $L-1$ veces, y luego aplique la función `linear_activation_forward` con la función SIGMOID para la salida (predicciones).\n",
    "\n",
    "<img src=\"images/model_architecture_kiank.png\" style=\"width:600px;height:300px;\">\n",
    "<caption><center> **Figure 2** : *[LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID* model</center></caption><br>\n",
    "\n",
    "**Ejercicio**: Implementar la propagación hacia adelante del modelo.\n",
    "\n",
    "**Instrucciones**: En el códigom la variable `AL` denotará $A^{[L]} = \\sigma(Z^{[L]}) = \\sigma(W^{[L]} A^{[L-1]} + b^{[L]})$.\n",
    "\n",
    "**Tips**:\n",
    "- Usa las funciones escritas anteriormente \n",
    "- Usa un bucle for con la función [LINEAR->RELU] (L-1) veces.\n",
    "- No olvides mantener registro de las caches en la lista. Para agregar un nuevo valor `c` a una lista puedes utilizar `list.append(c)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implementar la propagación hacia adelante [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID\n",
    "    \n",
    "    Argumentos:\n",
    "    X -- datos, arreglo de la forma (tamalo de la entrada, número de ejemplos)\n",
    "    parameters -- salida de la función initialize_parameters_deep()\n",
    "    \n",
    "    Regresa:\n",
    "    AL -- Último valor de activación (capa L)\n",
    "    caches -- Lista de caches con:\n",
    "                todas las caches de la función linear_activation_forward() (hay L-1, indexadas de 0 a L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # número de capas en la red neuronal\n",
    "    \n",
    "    # Implementar [LINEAR -> RELU]*(L-1). Agregar \"cache\" a la lista de \"caches\".\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        ### INICIO ###\n",
    "        W = parameters ['W'+str(l)]\n",
    "        b = parameters ['b' + str(l)]\n",
    "        A, cache = linear_activation_forward(A_prev, W, b, \"relu\")\n",
    "        caches.append(cache)\n",
    "        ### FIN ###\n",
    "    \n",
    "    # Implementar LINEAR -> SIGMOID. Agregar \"cache\" a la lista de \"caches\".\n",
    "    ### INICIO ### (≈ 2 lines of code)\n",
    "    AL, cache = linear_activation_forward(A, parameters ['W'+str(L)],parameters ['b' + str(L)],\"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    ### FIN ###\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL = [[0.03921668 0.70498921 0.19734387 0.04728177]]\n",
      "Tamaño de la lista de caches = 3\n"
     ]
    }
   ],
   "source": [
    "X, parameters = L_model_forward_test_case_2hidden()\n",
    "AL, caches = L_model_forward(X, parameters)\n",
    "print(\"AL = \" + str(AL))\n",
    "print(\"Tamaño de la lista de caches = \" + str(len(caches)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:50%\">\n",
    "  <tr>\n",
    "    <td> **AL** </td>\n",
    "    <td > [[ 0.03921668  0.70498921  0.19734387  0.04728177]]</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> **Length of caches list ** </td>\n",
    "    <td > 3 </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Función Costo\n",
    "\n",
    "Ya que implementamos la propagación hacia adelante, necesitamos calcular el costo para conocer si el modelo está realmente \"aprendiendo\"\n",
    "\n",
    "**Ejercicio**: Calcular el costo $J$, usando la siguiente fórmula: $$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right)) \\tag{7}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implementar la función de costo de la fórmula anterior.\n",
    "\n",
    "    Argumentos:\n",
    "    AL -- Matriz de probabilidades correspondiente a las predicciones de la forma (1, número de ejemplos)\n",
    "    Y -- Matriz de etiquetas verdaderas de la forma (1, número de ejemplos)\n",
    "\n",
    "    Regresa:\n",
    "    cost -- el cálculo de la función costo\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Calcular la pérdida con aL and y.\n",
    "    ### INICIO ###\n",
    "    cost = (-1/m) * (np.dot(Y, np.log(AL).T) + np.dot((1 - Y), np.log(1 - AL).T))\n",
    "    \n",
    "    ### FIN ###\n",
    "    \n",
    "    cost = np.squeeze(cost)      # Se asegura que la forma del costo es lo que esperamos (ejemplo, convierte [[17]] en 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 0.2797765635793422\n"
     ]
    }
   ],
   "source": [
    "Y, AL = compute_cost_test_case()\n",
    "\n",
    "print(\"cost = \" + str(compute_cost(AL, Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "<table>\n",
    "    <tr>\n",
    "    <td>**cost** </td>\n",
    "    <td> 0.2797765635793422</td> \n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Módulo de propagación hacia atrás\n",
    "\n",
    "Así como implementamos el módulo de propagación hacia adelante, crearemos funciones de ayuda para la propagación hacia atrás que nos permitan calcular el gradiente de la función costo con respecto a los parámetros. \n",
    "\n",
    "\n",
    "Se puede usar la regla de la cadena para calcular la derivada de la pérdida $\\mathcal{L}$ con respecto a $z^{[1]}$ en una red neuronal de 2 capas como sigue:\n",
    "\n",
    "$$\\frac{d \\mathcal{L}(a^{[2]},y)}{{dz^{[1]}}} = \\frac{d\\mathcal{L}(a^{[2]},y)}{{da^{[2]}}}\\frac{{da^{[2]}}}{{dz^{[2]}}}\\frac{{dz^{[2]}}}{{da^{[1]}}}\\frac{{da^{[1]}}}{{dz^{[1]}}} \\tag{8} $$\n",
    "\n",
    "Para calcular el gradiente $dW^{[1]} = \\frac{\\partial L}{\\partial W^{[1]}}$, Se usará la regla de la cadena anteior y se calculará $dW^{[1]} = dz^{[1]} \\times \\frac{\\partial z^{[1]} }{\\partial W^{[1]}}$. Durante la propagación hacia atrás, se multiplica el gradiente actual por el gradiente correspondiente a la capa específica que se desea.\n",
    "\n",
    "De manera equivalente, para calcular el gradiente $db^{[1]} = \\frac{\\partial L}{\\partial b^{[1]}}$, se usará la regla de la cadena anterior y se calcula $db^{[1]} = dz^{[1]} \\times \\frac{\\partial z^{[1]} }{\\partial b^{[1]}}$.\n",
    "\n",
    "Ahora de manera similar a la propagación hacia adelante, se construirá la propagación hacia atrás en tres pasos:\n",
    "- LINEAR backward\n",
    "- LINEAR -> ACTIVATION backward donde ACTIVATION calcula la derivada de las funciones de activación ReLU o sigmoid según corresponda.\n",
    "- [LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID backward (modelo completo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 - Linear backward\n",
    "\n",
    "Para la capa $l$, la parte lineal es: $Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$ (seguida de una activación).\n",
    "\n",
    "Supongamos que ya se ha calculado $dZ^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$. Se desea obtener $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$.\n",
    "\n",
    "<img src=\"images/linearback_kiank.png\" style=\"width:250px;height:300px;\">\n",
    "<caption><center> **Figure 4** </center></caption>\n",
    "\n",
    "Las tres salidas $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$ se calculan usando la entrada $dZ^{[l]}$.Las fórmulas necesarias son:\n",
    "$$ dW^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} \\tag{8}$$\n",
    "$$ db^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}\\tag{9}$$\n",
    "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \\tag{10}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Usa las 3 fórmulas anteriores para implementar linear_backward()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    # cache es \"linear_cache\" que contiene (A_prev, W, b) proveniente de la propagación hacia adelante en la capa actual.\n",
    "    \"\"\"\n",
    "    Implementar la parte lineal de la propagación hacia atrás para una sola capa (capa l)\n",
    "\n",
    "    Argumentos:\n",
    "    dZ -- Gradiente del costo con respecto a la salida lineal (de la capa actual l)\n",
    "    cache -- tupla de valores (A_prev, W, b) provenientes de la propagación hacia adelante en la capa actual.\n",
    "\n",
    "    Regresa:\n",
    "    dA_prev -- Gradiente del costo con respecto a la activación (de la capa anterior l-1) de la misma forma que A_prev\n",
    "    dW -- Gradiente del costo con respecto a W (de la capa actual l), De la misma forma que W\n",
    "    db -- Gradiente del costo con respecto a b (de la capa actual l), De la misma forma que b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    ### INICIO ### (≈ 3 lines of code)\n",
    "    dW = (1/m) * dZ @ A_prev.T\n",
    "    db = (1/m) * np.sum(dZ, axis = 1, keepdims = True)\n",
    "    dA_prev = W . T @ dZ\n",
    "    ### FIN ###\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_prev = [[-1.15171336  0.06718465 -0.3204696   2.09812712]\n",
      " [ 0.60345879 -3.72508701  5.81700741 -3.84326836]\n",
      " [-0.4319552  -1.30987417  1.72354705  0.05070578]\n",
      " [-0.38981415  0.60811244 -1.25938424  1.47191593]\n",
      " [-2.52214926  2.67882552 -0.67947465  1.48119548]]\n",
      "dW = [[ 0.07313866 -0.0976715  -0.87585828  0.73763362  0.00785716]\n",
      " [ 0.85508818  0.37530413 -0.59912655  0.71278189 -0.58931808]\n",
      " [ 0.97913304 -0.24376494 -0.08839671  0.55151192 -0.10290907]]\n",
      "db = [[-0.14713786]\n",
      " [-0.11313155]\n",
      " [-0.13209101]]\n"
     ]
    }
   ],
   "source": [
    "dZ, linear_cache = linear_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Salida esperada **:\n",
    "    \n",
    "```\n",
    "dA_prev = \n",
    " [[-1.15171336  0.06718465 -0.3204696   2.09812712]\n",
    " [ 0.60345879 -3.72508701  5.81700741 -3.84326836]\n",
    " [-0.4319552  -1.30987417  1.72354705  0.05070578]\n",
    " [-0.38981415  0.60811244 -1.25938424  1.47191593]\n",
    " [-2.52214926  2.67882552 -0.67947465  1.48119548]]\n",
    "dW = \n",
    " [[ 0.07313866 -0.0976715  -0.87585828  0.73763362  0.00785716]\n",
    " [ 0.85508818  0.37530413 -0.59912655  0.71278189 -0.58931808]\n",
    " [ 0.97913304 -0.24376494 -0.08839671  0.55151192 -0.10290907]]\n",
    "db = \n",
    " [[-0.14713786]\n",
    " [-0.11313155]\n",
    " [-0.13209101]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 - Linear-Activation backward\n",
    "\n",
    "A continuación crearemos una función que combine las dos funciones: **`linear_backward`** y el paso de propagación hacia atrás para la activación , a esta función le llamaremos **`linear_activation_backward`**. \n",
    "\n",
    "Para ayudar con la impementación de esta función se proveen dos funciones:\n",
    "- **`sigmoid_backward`**: Implementa la propagación hacia atrás de la función sigmoid.\n",
    "- **`relu_backward`**: Implementa la propagación hacia atrás de la función ReLu.\n",
    "\n",
    "If $g(.)$ es la función de activación, \n",
    "`sigmoid_backward` y `relu_backward` calculan $$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]}) \\tag{11}$$.  \n",
    "\n",
    "**Ejercicio**: Implementar la propagación hacia atrás para la función *LINEAR->ACTIVATION* de la capa actual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, cache):\n",
    "\n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, cache):\n",
    "\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # se copia dA ya que la derivada de ReLu es 1 para valores mayores que 0.\n",
    "    \n",
    "    # Se ajusta el valor para los valores de Relu menores que 0. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implementar la propagación hacia atrás para la función *LINEAR->ACTIVATION* de la capa actual.\n",
    "    \n",
    "    Argumentos:\n",
    "    dA -- gradiente de la función de activación para la capa actual\n",
    "    cache -- tupla de valores (linear_cache, activation_cache) almacenados para realizar de manera eficiente el cálculo de l propagación hacia atrás.\n",
    "    activation -- La activación a usar en esta capa en forma de cadena de texto: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Regresa:\n",
    "    dA_prev -- Gradiente del costo con respecto a la activación (de la capa anterior l-1), de la misma forma que A_prev\n",
    "    dW -- Gradiente del costo con respecto a W (capa actual l), de la misma forma que W\n",
    "    db -- Gradiente del costo con respecto a b (capa actual l), de la misma forma que b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        ### INICIO ###\n",
    "        dZ = relu_backward(dA, activation_cache) \n",
    "        \n",
    "        ### FIN ###\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        ### INICIO ###\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        ### FIN ###\n",
    "    \n",
    "    ### INICIO ###\n",
    "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    ### FIN ###\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid:\n",
      "dA_prev = [[ 0.11017994  0.01105339]\n",
      " [ 0.09466817  0.00949723]\n",
      " [-0.05743092 -0.00576154]]\n",
      "dW = [[ 0.10266786  0.09778551 -0.01968084]]\n",
      "db = [[-0.05729622]]\n",
      "\n",
      "relu:\n",
      "dA_prev = [[ 0.44090989  0.        ]\n",
      " [ 0.37883606  0.        ]\n",
      " [-0.2298228   0.        ]]\n",
      "dW = [[ 0.44513824  0.37371418 -0.10478989]]\n",
      "db = [[-0.20837892]]\n"
     ]
    }
   ],
   "source": [
    "dAL, linear_activation_cache = linear_activation_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation = \"sigmoid\")\n",
    "print (\"sigmoid:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db) + \"\\n\")\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation = \"relu\")\n",
    "print (\"relu:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salida esperada con sigmoid:**\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <td>\n",
    "        dA_prev\n",
    "     </td> \n",
    "     <td>\n",
    "         [[ 0.11017994  0.01105339]\n",
    "         [ 0.09466817  0.00949723]\n",
    "         [-0.05743092 -0.00576154]]\n",
    "      </td> \n",
    "  </tr> \n",
    "    <tr>\n",
    "        <td>\n",
    "            dW\n",
    "        </td> \n",
    "        <td>\n",
    "            [[ 0.10266786  0.09778551 -0.01968084]]\n",
    "        </td> \n",
    "  </tr> \n",
    "    <tr>\n",
    "        <td>\n",
    "        db\n",
    "       </td> \n",
    "       <td >\n",
    "           [[-0.05729622]]\n",
    "        </td> \n",
    "  </tr> \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salida Esperada con RELU:**\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <td > dA_prev </td> \n",
    "           <td > [[ 0.44090989  0.        ]\n",
    " [ 0.37883606  0.        ]\n",
    " [-0.2298228   0.        ]] </td> \n",
    "\n",
    "  </tr> \n",
    "  \n",
    "    <tr>\n",
    "    <td > dW </td> \n",
    "           <td > [[ 0.44513824  0.37371418 -0.10478989]] </td> \n",
    "  </tr> \n",
    "  \n",
    "    <tr>\n",
    "    <td > db </td> \n",
    "           <td > [[-0.20837892]] </td> \n",
    "  </tr> \n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 6.3 - Modelo de propagación hacia atrás.\n",
    "\n",
    "Ahora implementaresmos el modelo de la propagación haci atrás de la red neuronal completa. Recuerda que cuando se implementó la función `L_model_forward`, en cada iteración se guardó una caché que contiene (X,W,b, and z). En el módulo de propagación hacia atrás se usarán esos valores para calcular los gradientes. Por lo tanto en la función `L_model_backward` se iterará hacia atrás através de todas las capas comenzando desde la capa $L$. En cada paso se usarán los valores guardados de la capa $l$.\n",
    "\n",
    "<img src=\"images/mn_backward.png\" style=\"width:450px;height:300px;\">\n",
    "<caption><center>  **Figure 5** : Backward pass  </center></caption>\n",
    "\n",
    "** Inicializar la propagación hacia atrás ** :\n",
    "Para hacer la propagación hacia atrás através de la red, sabemos que la salida es,\n",
    "$A^{[L]} = \\sigma(Z^{[L]})$. El código debe calcular `dAL` $= \\frac{\\partial \\mathcal{L}}{\\partial A^{[L]}}$.\n",
    "Para hacerlo usa la fórmula siguiente:\n",
    "```python\n",
    "dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # Derivada del costo con respecto a AL\n",
    "```\n",
    "\n",
    "Podemos usar este gradiente `dAL` para seguir haciendo la propagación hacia atrás. Como se observa en la figura 5, ahora se puede introducir `dAL` en la función LINEAR->SIGMOID backward que se implementó antes(la cual usa los valores en caché guardados por el modelo de propagación hacia adelante). Posteriormente se deberá usar un ciclo `for` pára iterar através de todas las capas que usanr la función LINEAR->RELU backward. Deberás guardar cada dA, dW y db en el diccionario de gradientes usando la fórmula siguiente: \n",
    "\n",
    "$$grads[\"dW\" + str(l)] = dW^{[l]}\\tag{15} $$\n",
    "\n",
    "\n",
    "**Ejercicio**: Implementar la propagación haca atrás para el modelo *[LINEAR->RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implementar la propagación haca atrás para el modelo [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID.\n",
    "    \n",
    "    Argumentos:\n",
    "    AL -- Matriz de probabilidades arrojada por el modelo (L_model_forward())\n",
    "    Y -- Vector de etiquetas verdaderas (1 o 0)\n",
    "    caches -- Lista de caches que contenga:\n",
    "                todas las cachés de las funciones linear_activation_forward() con \"relu\" (caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                la caché de la función linear_activation_forward() con \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Regresa:\n",
    "    grads -- Un diccionario con los gradientes\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # número de capas\n",
    "    m = AL.shape[1] #el número de columnas AL corresponde al nùmero de ejemplos de entrenamiento\n",
    "    Y = Y.reshape(AL.shape) # Nos aseguramos de que la forma de la variable de etiquetas verdaderas sea igual a la de la salida de la red (AL)\n",
    "    \n",
    "    # Inicializar la propagación hacia atrás\n",
    "    ### INICIO ###\n",
    "    dAL = -(np.divide(Y, AL)- np.divide(1-Y, 1-AL))\n",
    "\n",
    "    \n",
    "    ### FIN ###\n",
    "    \n",
    "    # Gradientes para para la capa L (SIGMOID -> LINEAR). entradas: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    ### INICIO ###\n",
    "    current_cache = caches[L-1]\n",
    "    dAtemp, dWtemp, dbtemp = linear_activation_backward(dAL, current_cache, \"sigmoid\")\n",
    "    grads [\"dA\" + str(L - 1)] = dAtemp\n",
    "    grads[\"dW\"+ str(L)] = dWtemp\n",
    "    grads[\"db\" + str(L)] = dbtemp\n",
    "    ### FIN ### \n",
    "    \n",
    "    # bucle de l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)):\n",
    "        # Gradientes (RELU -> LINEAR).\n",
    "        # Entradas: \"grads[\"dA\" + str(l + 1)], current_cache\". Salidas: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        ### INICIO ###\n",
    "        current_cache = caches[l]\n",
    "        dA_prev = grads[\"dA\" + str(l + 1)]\n",
    "        dAtemp, dWtemp, dbtemp = linear_activation_backward(dA_prev, current_cache, \"relu\")\n",
    "        grads [\"dA\" + str(l)] = dAtemp\n",
    "        grads[\"dW\"+ str(l+1)] = dWtemp\n",
    "        grads[\"db\" + str(l+1)] = dbtemp\n",
    "        \n",
    "        ### FIN ###\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW1 = [[0.41010002 0.07807203 0.13798444 0.10502167]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.05283652 0.01005865 0.01777766 0.0135308 ]]\n",
      "db1 = [[-0.22007063]\n",
      " [ 0.        ]\n",
      " [-0.02835349]]\n",
      "dA1 = [[ 0.12913162 -0.44014127]\n",
      " [-0.14175655  0.48317296]\n",
      " [ 0.01663708 -0.05670698]]\n"
     ]
    }
   ],
   "source": [
    "AL, Y_assess, caches = L_model_backward_test_case()\n",
    "grads = L_model_backward(AL, Y_assess, caches)\n",
    "print_grads(grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**\n",
    "\n",
    "<table style=\"width:60%\">\n",
    "  \n",
    "  <tr>\n",
    "    <td > dW1 </td> \n",
    "           <td > [[ 0.41010002  0.07807203  0.13798444  0.10502167]\n",
    " [ 0.          0.          0.          0.        ]\n",
    " [ 0.05283652  0.01005865  0.01777766  0.0135308 ]] </td> \n",
    "  </tr> \n",
    "  \n",
    "    <tr>\n",
    "    <td > db1 </td> \n",
    "           <td > [[-0.22007063]\n",
    " [ 0.        ]\n",
    " [-0.02835349]] </td> \n",
    "  </tr> \n",
    "  \n",
    "  <tr>\n",
    "  <td > dA1 </td> \n",
    "           <td > [[ 0.12913162 -0.44014127]\n",
    " [-0.14175655  0.48317296]\n",
    " [ 0.01663708 -0.05670698]] </td> \n",
    "\n",
    "  </tr> \n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 - Actualizar parámetros\n",
    "\n",
    "En esta sección actualizaremos los parámetros del modelo usando descenso por gradiente.\n",
    "\n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{16}$$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{17}$$\n",
    "\n",
    "donde $\\alpha$ es la tasa de aprendizaje. Después de calcular los parámetros actualizados, guardarlos en un diccionario de parámetros. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**: Implementar `update_parameters()` para actualizar los parámetros usando descenso en gradiente.\n",
    "\n",
    "**Instrucciones**:\n",
    "Actualizar los parámetros usando descenso por gradiente en cada $W^{[l]}$ and $b^{[l]}$ para $l = 1, 2, ..., L$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Actualizar los parámetros usando descenso por gradiente\n",
    "    \n",
    "    Argumentos:\n",
    "    parameters -- diccionario con los parámetros.\n",
    "    grads -- diccionario con los gradientes provenientes de la función L_model_backward\n",
    "    \n",
    "    Regresa:\n",
    "    parameters -- Diccionario de python con los parámetros actualizados. \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # número de capas en la red neuronal\n",
    "\n",
    "    # Regla de actualización para cada parámetro. Usa un bucle for.\n",
    "    ### INICIO ###\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\"+str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\"+str(l+1)]\n",
    "        \n",
    "    ### FIN ###\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n",
      " [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n",
      " [-1.0535704  -0.86128581  0.68284052  2.20374577]]\n",
      "b1 = [[-0.04659241]\n",
      " [-1.28888275]\n",
      " [ 0.53405496]]\n",
      "W2 = [[-0.55569196  0.0354055   1.32964895]]\n",
      "b2 = [[-0.84610769]]\n"
     ]
    }
   ],
   "source": [
    "parameters, grads = update_parameters_test_case()\n",
    "parameters = update_parameters(parameters, grads, 0.1)\n",
    "\n",
    "print (\"W1 = \"+ str(parameters[\"W1\"]))\n",
    "print (\"b1 = \"+ str(parameters[\"b1\"]))\n",
    "print (\"W2 = \"+ str(parameters[\"W2\"]))\n",
    "print (\"b2 = \"+ str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table style=\"width:100%\"> \n",
    "    <tr>\n",
    "    <td > W1 </td> \n",
    "           <td > [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n",
    " [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n",
    " [-1.0535704  -0.86128581  0.68284052  2.20374577]] </td> \n",
    "  </tr> \n",
    "  \n",
    "    <tr>\n",
    "    <td > b1 </td> \n",
    "           <td > [[-0.04659241]\n",
    " [-1.28888275]\n",
    " [ 0.53405496]] </td> \n",
    "  </tr> \n",
    "  <tr>\n",
    "    <td > W2 </td> \n",
    "           <td > [[-0.55569196  0.0354055   1.32964895]]</td> \n",
    "  </tr> \n",
    "  \n",
    "    <tr>\n",
    "    <td > b2 </td> \n",
    "           <td > [[-0.84610769]] </td> \n",
    "  </tr> \n",
    "</table>\n"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "c4HO0",
   "launcher_item_id": "lSYZM"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
